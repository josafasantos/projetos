{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL da pagina web a ser raspada (Arquivo HTML local)\n",
    "URL_PAGINA_LIVROS = 'html_exemplo/pagina_livros_exemplo.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria função para raspar dados do site\n",
    "def raspar_dados_livros(url):\n",
    "  \"\"\"\n",
    "    Raspa dados de livros de uma página web estática.\n",
    "\n",
    "    Args:\n",
    "      url (str): URL da página web\n",
    "\n",
    "    Returns:\n",
    "      list: Lista de dicionários, onde cada dicionário representa um livro\n",
    "      com chaves 'titulo', 'autor', 'preco'\n",
    "      Retorna um lista vazia em caso de erro\n",
    "  \"\"\"\n",
    "  \n",
    "  try: \n",
    "    # Faz a requisição HTTP GET\n",
    "    #response = requests.get(url)\n",
    "    # Lança uma exceção para status de erro (4xx ou 5xx)\n",
    "    #response.raise_for_status()\n",
    "\n",
    "    # Leitura arquivo HTML local\n",
    "    with open(URL_PAGINA_LIVROS, 'r', encoding='utf-8') as arquivo_html:\n",
    "      response = arquivo_html.read()\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erro ao acessar página: {e}\")\n",
    "    # Retorna lista vazia em caso de erro de requisição\n",
    "    return []\n",
    "  \n",
    "  # Parseia o HTML HTTP\n",
    "  #soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  # Parseia o HTML Local\n",
    "  soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "  # Lista para armazenar os dados dos livros\n",
    "  livros_data = []\n",
    "\n",
    "  # Encontrar todos os elementos 'div' que representam um item de livro\n",
    "  itens_livro = soup.find_all('div', class_='livro-item')\n",
    "\n",
    "  # Cria o dicionário com os livros\n",
    "  for item_livro in itens_livro:\n",
    "    try:\n",
    "      titulo_elemento = item_livro.find('h2', class_='titulo-livro')\n",
    "      autor_elemento = item_livro.find('p', class_='autor-livro')\n",
    "      preco_elemento = item_livro.find('p', class_='preco-livro')\n",
    "\n",
    "      # Extrair o texto dos elementos\n",
    "      titulo = titulo_elemento.text.strip() if titulo_elemento else 'N/A'\n",
    "      autor = autor_elemento.text.strip() if autor_elemento else 'N/A'\n",
    "      preco = preco_elemento.text.strip() if preco_elemento else 'N/A'\n",
    "\n",
    "      # Criar um dicionário com os dados do livro\n",
    "      livro_info = {\n",
    "        'titulo': titulo,\n",
    "        'autor': autor,\n",
    "        'preco': preco\n",
    "      }\n",
    "\n",
    "      # Adicionar o dicionário à lista\n",
    "      livros_data.append(livro_info)\n",
    "    \n",
    "    # Capturar erros se algum elemento não for encontrado\n",
    "    except AttributeError as e:\n",
    "      print(f'Erro ao extrair dados de um item de livro: {e}')\n",
    "      # Ir para o próximo item de livro em caso de erro\n",
    "      continue\n",
    "  return livros_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dados em CSV\n",
    "def salvar_dados_csv(dados_livros, nome_arquivo_csv='dados_coletados/livros_csv'):\n",
    "  \"\"\"\"\n",
    "  Salva uma lista de dicionários de dados de livros em um arquivo CSV\n",
    "\n",
    "  Args:\n",
    "    dados_livros (list): Lista de dicionários, onde cada dicionário representa um livro.\n",
    "    nome_arquivo_csv (str): nome do arquivo CSV para salvar\n",
    "  \"\"\"\n",
    "  # Verifica se há dados para salvar\n",
    "  if not dados_livros:\n",
    "    print(\"Nenhum dado de livro para salvar\")\n",
    "    return\n",
    "  \n",
    "  # Cria DataFrame a partir da lista de dicionários\n",
    "  df_livros = pd.DataFrame(dados_livros)\n",
    "\n",
    "  try:\n",
    "    # Salvar DataFrame para CSV\n",
    "    df_livros.to_csv(nome_arquivo_csv, index=False, encoding='utf-8')\n",
    "    print(f'Dados dos livros salvos com sucesso em \"{nome_arquivo_csv}\"')\n",
    "  except Exception as e:\n",
    "    print(f\"Erro ao salvar dados em CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados dos livros salvos com sucesso em \"dados_coletados/livros_csv\"\n",
      "Processo de web scraping concluído.\n"
     ]
    }
   ],
   "source": [
    "dados_livros_raspados = raspar_dados_livros(URL_PAGINA_LIVROS)\n",
    "# Verifica se dados foram raspados com sucesso\n",
    "if dados_livros_raspados:\n",
    "  # Chama função para salvar em CSV\n",
    "  salvar_dados_csv(dados_livros_raspados)\n",
    "  print(\"Processo de web scraping concluído.\")\n",
    "else:\n",
    "  print(\"Raspagem de dados falhou\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
