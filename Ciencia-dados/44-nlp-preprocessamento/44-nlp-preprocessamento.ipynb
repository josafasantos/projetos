{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização e Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar recursos\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega modelo SpaCy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de pré-processamento com lematização\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocessar_com_lematizacao(texto):\n",
    "  texto = texto.lower()\n",
    "  texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "  doc = nlp(texto)\n",
    "  tokens = [token.lemma_ for token in doc if token.text not in stopwords_pt and not token.is_space]\n",
    "  return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de pré-processamento sem lematização\n",
    "def preprocessar_sem_lematizacao(texto):\n",
    "  texto = texto.lower()\n",
    "  texto = texto.translate(str.maketrans('','',string.punctuation))\n",
    "  tokens = word_tokenize(texto, language='portuguese')\n",
    "  tokens = [word for word in tokens if word not in stopwords_pt]\n",
    "  return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df = pd.read_csv('../../datasets/dataset_exemplo_nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar os dois tipos de pré-processamento\n",
    "df['texto_sem_lematizacao'] = df['texto'].apply(preprocessar_sem_lematizacao)\n",
    "df['texto_com_lematizacao'] = df['texto'].apply(preprocessar_com_lematizacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorização\n",
    "vectorizer = CountVectorizer()\n",
    "X_sem = vectorizer.fit_transform(df['texto_sem_lematizacao'])\n",
    "X_com = vectorizer.fit_transform(df['texto_com_lematizacao'])\n",
    "y = df['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão treino/teste\n",
    "X_train_sem, X_test_sem, y_train, y_test = train_test_split(X_sem, y, test_size=0.3, random_state=42)\n",
    "X_train_com, X_test_com, _,_ = train_test_split(X_com, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo sem lematização\n",
    "modelo_sem = LogisticRegression()\n",
    "modelo_sem.fit(X_train_sem, y_train)\n",
    "y_pred_sem = modelo_sem.predict(X_test_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com lematização\n",
    "modelo_com = LogisticRegression()\n",
    "modelo_com.fit(X_train_com, y_train)\n",
    "y_pred_com = modelo_com.predict(X_test_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempenho SEM lematização\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       1.00      1.00      1.00        26\n",
      "    positivo       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resultados\n",
    "print('Desempenho SEM lematização')\n",
    "print(classification_report(y_test, y_pred_sem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempenho COM lematização\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       1.00      1.00      1.00        26\n",
      "    positivo       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Desempenho COM lematização')\n",
    "print(classification_report(y_test, y_pred_com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frase: Achei o atendimento muito demorado e desorganizado\n",
      "→ Classificação SEM lematização: negativo\n",
      "→ Classificação COM lematização: positivo\n"
     ]
    }
   ],
   "source": [
    "# Testar frase nova\n",
    "frase_nova = \"Achei o atendimento muito demorado e desorganizado\"\n",
    "\n",
    "# Pré-processar com e sem lematização\n",
    "frase_sem_lematizacao = preprocessar_sem_lematizacao(frase_nova)\n",
    "frase_com_lematizacao = preprocessar_com_lematizacao(frase_nova)\n",
    "\n",
    "# Vetorizar (usando o mesmo vectorizer treinado anteriormente!)\n",
    "X_nova_sem = vectorizer.transform([frase_sem_lematizacao])\n",
    "X_nova_com = vectorizer.transform([frase_com_lematizacao])\n",
    "\n",
    "# Fazer previsão\n",
    "pred_sem = modelo_sem.predict(X_nova_sem)[0]\n",
    "pred_com = modelo_com.predict(X_nova_com)[0]\n",
    "\n",
    "print(f\"\\nFrase: {frase_nova}\")\n",
    "print(f\"→ Classificação SEM lematização: {pred_sem}\")\n",
    "print(f\"→ Classificação COM lematização: {pred_com}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
